{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fe30298-9f8d-47f1-a91d-2426cbe08f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import datetime as datetime\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import docx2txt\n",
    "from typing import List, Dict, Tuple, Sequence, Optional\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a345038-2bfc-4e03-a32e-400cba23f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "_TOKENIZER = None\n",
    "\n",
    "_CLEAN_PUA = re.compile(r'[\\uE000-\\uF8FF]')                 # private-use (e.g., \\uf0b7)\n",
    "_CLEAN_ZW  = re.compile(r'[\\u200B-\\u200D\\uFEFF]')           # zero-widths/BOM\n",
    "_CLEAN_CTRL= re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]')    # other control chars\n",
    "_SOFT_HY   = re.compile(r'\\u00AD')                          # soft hyphen\n",
    "_LINE_HY   = re.compile(r'-\\s*\\n\\s*')                       # hyphen line-break joins\n",
    "_BULLETS   = re.compile(r'[\\u2022\\u00B7]')                  # • or ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba50c4c4-edf7-45d2-a603-5f52d31ae594",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4100cc0-ae8d-4aa6-9a9e-25da3dfd91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iso_time():\n",
    "    now = datetime.now(timezone.utc)\n",
    "    timestamp_iso = now.isoformat(timespec=\"milliseconds\")\n",
    "    timestamp_iso = timestamp_iso.replace(\"+00:00\", \"Z\")\n",
    "    timestamp_ms = int(now.timestamp()*1000)\n",
    "\n",
    "    return timestamp_iso, timestamp_ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb304d6-c1ca-4438-93b6-0ef151ff2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uid(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d7765f-647b-47d0-be6b-142047a7655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_embedding(text, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(embedding_model)\n",
    "    return model.encode(text, normalize_embeddings=True) #return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1f9713-2ebb-496b-a33d-7efc718a4756",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is an existing row in the input history for user where the same input was already used on the same model\n",
    "#returns row where user_input and model combination already exist\n",
    "def check_existing_input(user_input, prompt_model, filename):\n",
    "    if not os.path.isfile(filename):\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    matches = df.loc[(df['prompt_model'] == prompt_model) & (df['user_input'] == user_input)].copy()\n",
    "\n",
    "    return matches if not matches.empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b7d9d17-8d41-43c1-9532-874f4312503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_input_embedding(user_hash, user_input, input_uid, embedding_model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    filename = user_hash+\"_InputEmbeddings.pkl\"\n",
    "    embedding = create_vector_embedding(user_input, embedding_model)\n",
    "\n",
    "    new_row = {\n",
    "        \"uid\" : input_uid,\n",
    "        \"embedding\" : embedding\n",
    "    }\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(new_row)\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07aceaf9-20ff-4640-a76e-cb36b8e2c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_input_embeddings(user, embedding_model='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    user_hash = get_uid(user)\n",
    "    filename = user_hash+\"_InputHistory.csv\"\n",
    "    temp_file = user_hash+\"InputHistory_temp.csv\"\n",
    "    changed = False\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        print(f\"File does not exist for user: {user}\")\n",
    "        return False\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        filename,\n",
    "        dtype={\n",
    "            \"embedding_exists\" : \"int64\",\n",
    "            \"embedding_model\" : \"string\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['embedding_exists'] == 0:\n",
    "            save_input_embedding(user_hash, row['user_input'], row['input_uid'], embedding_model)\n",
    "            df.at[index, 'embedding_exists'] = 1\n",
    "            df.at[index, 'embedding_model'] = embedding_model\n",
    "            df.to_csv(temp_file, index=False)\n",
    "            changed = True\n",
    "\n",
    "    if changed:\n",
    "        os.replace(temp_file, filename)\n",
    "        print(\"Updates written to file\")\n",
    "\n",
    "    else:\n",
    "        print(\"No updates written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4c333f-f0e7-486f-b6d2-06c67919ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_embeddings(user):\n",
    "    user_hash = get_uid(user)\n",
    "    filename = user_hash+\"_InputEmbeddings.pkl\"\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc672d07-92f1-47f3-8c56-ea98858fb160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_user_input(user, user_input, prompt_model):\n",
    "    user_hash = get_uid(user)\n",
    "    input_uid = get_uid(user_input)\n",
    "    filename = user_hash+\"_InputHistory.csv\"\n",
    "    \n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                'user_input',\n",
    "                'input_uid',\n",
    "                'prompt_model',\n",
    "                'timestamp_iso',\n",
    "                'timestamp_ms',\n",
    "                'embedding_exists',\n",
    "                'embedding_model',\n",
    "                'processed'\n",
    "            ])\n",
    "\n",
    "    existing_row = check_existing_input(user_input, prompt_model, filename)\n",
    "    \n",
    "    if existing_row is not None:\n",
    "        dt = existing_row['timestamp_ms'].iloc[0]\n",
    "        dt = datetime.fromtimestamp(dt / 1000)\n",
    "        print(f\"Input already used at: {dt}\")\n",
    "        return False\n",
    "\n",
    "    timestamp_iso, timestamp_ms = get_iso_time()\n",
    "\n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\n",
    "            user_input,\n",
    "            input_uid,\n",
    "            prompt_model,\n",
    "            timestamp_iso,\n",
    "            timestamp_ms,\n",
    "            0, #flag for if a vector embedding exists for the text, default=0\n",
    "            \"None\", #placeholder for vector embedding model used\n",
    "            0 #flag for if the text has been processed by LLM\n",
    "        ])\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd1702c-ccba-473e-8a29-4334c9451b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tokenizer(name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    global _TOKENIZER\n",
    "    if _TOKENIZER is None:\n",
    "        _TOKENIZER = AutoTokenizer.from_pretrained(name, use_fast=True)\n",
    "    return _TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e90f42-e4dc-4f07-95f5-fd38064a0e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tok_len(tok, s: str) -> int:\n",
    "    return len(tok.encode(s, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d359417-1ed6-459e-99f8-0da78398b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = _LINE_HY.sub('-', text)          # join hyphenated line breaks\n",
    "    text = _SOFT_HY.sub('', text)           # drop soft hyphen\n",
    "    text = _CLEAN_ZW.sub('', text)          # remove zero-widths\n",
    "    text = _CLEAN_PUA.sub('', text)         # remove private-use (incl. \\uf0b7)\n",
    "    text = _CLEAN_CTRL.sub(' ', text)       # drop stray controls\n",
    "    text = _BULLETS.sub(' • ', text)        # normalize bullets if you want to keep them\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3948c7c-3dd9-4210-bbda-eb81362294b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_sentence_split(text: str) -> List[Dict]:\n",
    "    text = clean_text(text)\n",
    "    _SPLIT = re.compile(r'(?<=[.!?])\\s+(?=[A-Z0-9\"\\'(])')\n",
    "    _ABBR_END = re.compile(r'\\b(e\\.g|i\\.e|Mr|Ms|Dr)\\.$')\n",
    "    norm = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if not norm:\n",
    "        return []\n",
    "    parts, start = [], 0\n",
    "    for m in _SPLIT.finditer(norm):\n",
    "        parts.append((start, m.start()))\n",
    "        start = m.end()\n",
    "    parts.append((start, len(norm)))\n",
    "\n",
    "    joined: List[Dict] = []\n",
    "    for s, e in parts:\n",
    "        seg = norm[s:e]\n",
    "        if not seg:\n",
    "            continue\n",
    "        if joined and _ABBR_END.search(joined[-1][\"text\"]):\n",
    "            joined[-1][\"text\"] = norm[joined[-1][\"start\"]:e]\n",
    "            joined[-1][\"end\"] = e\n",
    "        else:\n",
    "            joined.append({\"text\": seg, \"start\": s, \"end\": e})\n",
    "    return joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "072efe56-c469-443a-8bf1-dccd7f93b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_overlong_unit(u: Dict, tok, max_tokens: int) -> List[Dict]:\n",
    "    enc = tok(u[\"text\"], add_special_tokens=False, return_offsets_mapping=True)\n",
    "    ids = enc[\"input_ids\"]\n",
    "    offs = enc[\"offset_mapping\"]  # [(start,end) in u[\"text\"]]\n",
    "    out: List[Dict] = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        j = min(i + max_tokens, len(ids))\n",
    "        sub_rel_start = offs[i][0]\n",
    "        sub_rel_end   = offs[j-1][1]\n",
    "        sub_text = u[\"text\"][sub_rel_start:sub_rel_end]\n",
    "        out.append({\n",
    "            \"text\": sub_text,\n",
    "            \"start\": u[\"start\"] + sub_rel_start,\n",
    "            \"end\":   u[\"start\"] + sub_rel_end,\n",
    "            **{k: v for k, v in u.items() if k not in (\"text\",\"start\",\"end\")}\n",
    "        })\n",
    "        i = j\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9815f37e-e878-4afa-b09f-facdbbdc9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_chunks(\n",
    "    units: Sequence[Dict],\n",
    "    tokenizer_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    max_tokens: int = 240,\n",
    "    overlap_tokens: int = 48,\n",
    "    corpus: Optional[str] = None,\n",
    "    carry_keys: Sequence[str] = (\"page\",)\n",
    "    ) -> List[Dict]:\n",
    "    \n",
    "    tok = _get_tokenizer(tokenizer_name)\n",
    "\n",
    "    # 1) Expand any overlong sentences\n",
    "    expanded: List[Dict] = []\n",
    "    src_idx: List[int] = []\n",
    "    for i, u in enumerate(units):\n",
    "        if _tok_len(tok, u[\"text\"]) > max_tokens:\n",
    "            parts = _split_overlong_unit(u, tok, max_tokens)\n",
    "            expanded.extend(parts)\n",
    "            src_idx.extend([i] * len(parts))\n",
    "        else:\n",
    "            expanded.append(u.copy())\n",
    "            src_idx.append(i)\n",
    "\n",
    "    # 2) Precompute token lengths\n",
    "    for u in expanded:\n",
    "        u[\"_tok\"] = _tok_len(tok, u[\"text\"])\n",
    "\n",
    "    # 3) Greedy pack with overlap\n",
    "    chunks: List[Dict] = []\n",
    "    cur: List[Dict] = []\n",
    "    cur_src: List[int] = []\n",
    "    cur_tok = 0\n",
    "\n",
    "    def flush():\n",
    "        nonlocal cur, cur_src, cur_tok\n",
    "        if not cur:\n",
    "            return\n",
    "        s0, eN = cur[0][\"start\"], cur[-1][\"end\"]\n",
    "        text = corpus[s0:eN] if corpus is not None else \" \".join(u[\"text\"] for u in cur).strip()\n",
    "\n",
    "        meta = {}\n",
    "        for k in carry_keys:\n",
    "            vals = [u.get(k) for u in cur if u.get(k) is not None]\n",
    "            if vals:\n",
    "                meta[k + \"s\"] = sorted(set(vals))  # e.g., 'pages'\n",
    "\n",
    "        chunks.append({\n",
    "            \"id\": len(chunks),\n",
    "            \"text\": text,\n",
    "            \"start\": s0,\n",
    "            \"end\": eN,\n",
    "            \"n_tokens\": cur_tok,\n",
    "            \"unit_indices\": sorted(set(cur_src)),\n",
    "            **meta\n",
    "        })\n",
    "\n",
    "        # build overlap tail\n",
    "        if overlap_tokens > 0:\n",
    "            t = 0\n",
    "            tail, tail_src = [], []\n",
    "            for u, si in zip(reversed(cur), reversed(cur_src)):\n",
    "                tail.insert(0, u); tail_src.insert(0, si)\n",
    "                t += u[\"_tok\"]\n",
    "                if t >= overlap_tokens:\n",
    "                    break\n",
    "            cur, cur_src = tail, tail_src\n",
    "            cur_tok = sum(u[\"_tok\"] for u in cur)\n",
    "        else:\n",
    "            cur, cur_src, cur_tok = [], [], 0\n",
    "\n",
    "    for u, si in zip(expanded, src_idx):\n",
    "        if cur and cur_tok + u[\"_tok\"] > max_tokens:\n",
    "            flush()\n",
    "        cur.append(u); cur_src.append(si); cur_tok += u[\"_tok\"]\n",
    "    flush()\n",
    "\n",
    "    # optional: drop temp fields\n",
    "    for u in expanded:\n",
    "        u.pop(\"_tok\", None)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea11cf3-1c92-4413-891d-c04e2b9bf21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk_to_parquet(chunk_uid, text, filename=\"library.parquet\"):\n",
    "    new_row = pd.DataFrame([{\n",
    "        \"chunk_uid\" : chunk_uid,\n",
    "        \"text\" : text\n",
    "    }])\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        new_row.to_parquet(filename, engine=\"fastparquet\", index=False)\n",
    "\n",
    "    else:\n",
    "        df = pd.read_parquet(filename, engine=\"fastparquet\")\n",
    "\n",
    "        if chunk_uid not in df[\"chunk_uid\"].values:\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "            df.to_parquet(filename, engine=\"fastparquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d786e87f-a14c-4389-878a-796a5429a9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vector(chunk_uid, vector, filename=\"library.pkl\"):\n",
    "    new_row = {\n",
    "        \"chunk_uid\" : chunk_uid,\n",
    "        \"vector_embedding\" : vector\n",
    "    }\n",
    "\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, \"wb\") as f:\n",
    "            pickle.dump([new_row], f)\n",
    "        return\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        library = pickle.load(f)\n",
    "\n",
    "    for row in library:\n",
    "        if row[\"chunk_uid\"] == chunk_uid:\n",
    "            if np.allclose(row[\"vector_embedding\"], new_row[\"vector_embedding\"]):\n",
    "                return\n",
    "            else:\n",
    "                row[\"vector_embedding\"] = new_row[\"vector_embedding\"]\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    pickle.dump(library, f)\n",
    "                return\n",
    "\n",
    "    library.append(new_row)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(library, f)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aed1c95-601b-45f1-b558-d866ebbad4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docx(file_path):\n",
    "    return docx2txt.process(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66cad126-cf86-4faf-971d-6d02f6a0039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    \n",
    "    page_texts = []\n",
    "\n",
    "    for p in range(len(doc)):\n",
    "        raw = doc[p].get_text(\"text\")\n",
    "        page_texts.append(clean_text(raw))\n",
    "\n",
    "        SEP = \"\\n\\n\"\n",
    "        corpus = SEP.join(page_texts)\n",
    "\n",
    "        units = []\n",
    "        offset = 0\n",
    "\n",
    "        for page_idx, page_clean in enumerate(page_texts, start=1):\n",
    "            sentences = simple_sentence_split(page_clean)\n",
    "            for s in sentences:\n",
    "                units.append({\n",
    "                    \"text\" : s[\"text\"],\n",
    "                    \"start\" : offset + s[\"start\"],\n",
    "                    \"end\" : offset + s[\"end\"],\n",
    "                    \"page\" : page_idx\n",
    "                })\n",
    "                \n",
    "    return corpus, units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "261112d1-3978-4c99-958e-e61ad8e5b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus, units = parse_pdf(\"2001.00973v1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e87b84b-0d25-42f1-9b6a-27dc2a6ce7df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pack_chunks(units, corpus)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32eec8be-b117-4496-b7a7-d4ed8a29a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docx(file_path):\n",
    "    text = parse_docx(file_path) or \"\"\n",
    "    sentences = simple_sentence_split(text)\n",
    "\n",
    "    chunks = pack_chunks(\n",
    "        sentences,\n",
    "        tokenizer_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        max_tokens=240,\n",
    "        overlap_tokens=48,\n",
    "        corpus=None,\n",
    "        carry_keys=(\"page\",)\n",
    "    )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0c9491ab-d4d8-4566-ab92-c9a068b4ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path):\n",
    "    corpus, units = parse_pdf(file_path)\n",
    "    chunks = pack_chunks(units, corpus)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c00ac4c-1a13-4ed4-81f4-0362b0222902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(ext):\n",
    "    document_pipeline = {\n",
    "        \".pdf\": process_pdf,\n",
    "        \".docx\": process_docx,\n",
    "        \".txt\": None, \n",
    "        \".md\": None, \n",
    "        \".csv\": None, \n",
    "        \".xlsx\": None, \n",
    "        \".pptx\": None,\n",
    "        \".rtf\": None, \n",
    "        \".epub\": None, \n",
    "        \".odt\": None, \n",
    "        \".ods\": None, \n",
    "        \".odp\": None,\n",
    "        \".html\": None, \n",
    "        \".json\": None, \n",
    "        \".yml\": None, \n",
    "        \".eml\": None,\n",
    "    }\n",
    "    return document_pipeline.get(ext.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17e874-300c-403a-b81c-306f849fa3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model():\n",
    "    return SentenceTransformer(MODEL_NAME, cache_folder=\"./models_cache\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1240b311-0dbc-4595-8a6d-70abac5306a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_embedding(text, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    model = SentenceTransformer(embedding_model,\n",
    "                               cache_folder=\"./models_cache\"\n",
    "                               )\n",
    "    \n",
    "    vector = model.encode(text, convert_to_numpy=True, batch_size=32, show_progress_bar=False)\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6934ec18-6dbe-4466-90da-88d1ffd371b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, **kwargs):\n",
    "    embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    library_csv = \"library.csv\"\n",
    "    vector_pkl = \"library_vectors.pkl\"\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "    pipeline = get_pipeline(ext)\n",
    "\n",
    "    model = SentenceTransformer(embedding_model,\n",
    "                                cache_folder=\"./models_cache\"\n",
    "                               )\n",
    "    \n",
    "    if pipeline is None:\n",
    "        raise ValueError(f\"Unsupported file type: {ext}\")\n",
    "        \n",
    "    chunks = pipeline(file_path, **kwargs)\n",
    "\n",
    "    if not os.path.isfile(library_csv):\n",
    "        with open(library_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                \"chunk_uid\",\n",
    "                \"document\",\n",
    "                \"document_type\",\n",
    "                \"n_tokens\",\n",
    "                \"embedding_model\",\n",
    "                \"character_start\",\n",
    "                \"character_end\",\n",
    "                \"timestamp_iso\",\n",
    "                \"timestamp_ms\"\n",
    "            ])\n",
    "\n",
    "    c = 0\n",
    "    for chunk in chunks:\n",
    "        chunk_uid = get_uid(chunk['text'])\n",
    "        \n",
    "        #check if row already exists\n",
    "        df = pd.read_csv(library_csv)\n",
    "        if not df.loc[df['chunk_uid'] == chunk_uid].empty:\n",
    "            continue\n",
    "\n",
    "        vector = model.encode(chunk['text'], convert_to_numpy=True, batch_size=32, show_progress_bar=False)\n",
    "        \n",
    "        timestamp_iso, timestamp_ms = get_iso_time()\n",
    "        \n",
    "        with open(library_csv, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\n",
    "                chunk_uid,\n",
    "                Path(file_path).stem,\n",
    "                ext,\n",
    "                chunk['n_tokens'],\n",
    "                MODEL_NAME,\n",
    "                chunk['start'],\n",
    "                chunk['end'],\n",
    "                timestamp_iso,\n",
    "                timestamp_ms\n",
    "            ])\n",
    "\n",
    "        #save the raw text and the uid into a parquet file\n",
    "        save_chunk_to_parquet(chunk_uid, chunk['text'])\n",
    "        save_vector(chunk_uid, vector)\n",
    "        \n",
    "        c += 1\n",
    "            \n",
    "    print(f\"{c} new chunks added to the library\")\n",
    "                \n",
    "    #return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33d1a113-efc2-48f7-b57a-906d63d6de18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input already used at: 2025-08-31 17:44:57.977000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = \"TylerTwohig\"\n",
    "user_input = \"z\"\n",
    "prompt_model = \"DeepSeek-R1:latest\"\n",
    "save_user_input(\"TylerTwohig\", user_input, prompt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d82ed34-ea1a-4269-8a68-af3ba82c90b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 new chunks added to the library\n"
     ]
    }
   ],
   "source": [
    "process_document(\"2001.00973v1.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0821e5dc-452a-44c6-bae4-aa2b0232e288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Introduction: The problem of data gathering — Asymmetries of power and knowledge What is obfuscation? Supermarkets and grocery chains have always been in the data business, as well as the food business: with small profit margins and a product that can quickly spoil, they pay close attention to inventory, purchasing patterns, and geography. The introduction of store “loyalty cards” perfectly fit a decades–long pattern: rewarding loyal customers with additional discounts in return for better data, which could inform mailings, coupon campaigns, even which products to shelve together. So far, so normal — but the appearance of “loyalty cards,” with their rather sinister Orwellian name, and direct connection of data collection with access to sales and discounts, sparked a strange revolt. Customers engaged in boycotts and tongue–in–cheek protests, but as loyalty cards became more common, and apparently permanent, strategies appeared to mitigate the perceived loss of privacy without entirely giving up the cards, and therefore the savings.'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"library.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "#for text in df['text']:\n",
    "#    print(text, \"\\n\")\n",
    "\n",
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6661f5b-f9b4-4c0c-b93e-d9877e6944d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"library.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ece934e4-6df9-403d-8ea8-502c3e43e750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b851ebe-f5c6-4b46-be87-d42fd1d166b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
